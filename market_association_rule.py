# -*- coding: utf-8 -*-
"""Market-Association-Rule.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-pxnaVbVfMHvl1UcJV6azEju1686_dT

#About the Dataset
##1. Introduction

In this document, we explore a dataset designed for association rules mining, a key technique in market basket analysis. The dataset includes various items typically found in retail transactions. Each item is represented as a binary variable, with "1" meaning the item is present and "0" meaning it is absent in individual transactions.

##2. Dataset Overview



Our dataset unfolds as a rich tapestry of distinct columns, each dedicated to representing a specific item:

* Bread
* Honey
* Bacon
* Toothpaste
* Banana
* Apple
* Hazelnut
* Cheese
* Meat
* Carrot
* Cucumber
* Onion
* Milk
* Butter
* ShavingFoam
* Salt
* Flour
* HeavyCream
* Egg
* Olive
* Shampoo
* Sugar

##3. Purpose of the Dataset
The purpose of this dataset is to uncover complex associations and patterns hidden within the network of customer transactions. Each row in the dataset represents a single transaction, and the values in each column indicate whether a particular item was included in the transaction or not.

##4. Data Format
The data in this repository is presented in binary form: "1" means an item was purchased, and "0" means it was not. This simple binary format highlights whether an item is present in a transaction, rather than how many of each item were bought.

#Importing Necessary Libraries
"""

import pandas as pd #Pandas is a powerful library for data manipulation and analysis.
import numpy as np #NumPy is a powerful tool for numerical computations in Python.
import matplotlib.pyplot as plt #Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns #Seaborn is a statistical data visualization library based on Matplotlib.
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

"""####The code df = pd.read_csv('market.csv') reads the data from the file market.csv and loads it into a DataFrame named df. We can then use this DataFrame to analyze and manipulate the data."""

markt= pd.read_csv('market.csv') #Reading the data.

markt #Displays the actual data present in the csv file.

"""####So looking in the above data frame we can see that the data maintained is not in the correct order as the names of the items i.e columns are all in one cell, and as well as the data present in the rows are also in one single cell, making it very hard to extract any insights from the data set."""

markt.columns #Displays the names of the columns.

"""####As we can check the names of the items in the columns are in the same cell and only separated by the semicolon ';', same is with the following rows in the same cell, so we have clean and separate the data as we can go through the rows and column and can work on data manipulation and data analysis."""

file_path = 'market.csv' #This line assigns the string 'market.csv' to the variable file_path
with open(file_path, 'r') as file: #open(file_path, 'r'): This function opens the file specified by file_path in read mode ('r'). Opening a file in read mode means you can only read its contents, not modify it.
    lines = file.readlines() #file.readlines(): This method reads all the lines from the file and returns them as a list of strings.

# The first line is the header
header = lines[0].strip().split(';')

"""###The line header = lines[0].strip().split(';') processes the first line of the file to extract and format the column names.

###lines[0]:

####This accesses the first line of the list lines, which was created by file.readlines(). In the context of a CSV file, this first line typically contains the column headers.

###.split(';'):

####The split(';') method divides the string into a list of substrings based on the delimiter specifiedâ€”in this case, a semicolon (;).

###header =:

####This assigns the resulting list of column names to the variable header.
"""

# The rest are the data rows
data = [line.strip().split(';') for line in lines[1:]]

# Create a DataFrame
df = pd.DataFrame(data, columns=header)

"""###The line data = [line.strip().split(';') for line in lines[1:]] is a Python list comprehension that processes the lines of a CSV file to extract and format the data

###lines[1:]:

####lines: A list of strings where each string represents a line from the CSV file.

###for line in lines[1:]:

####This part of the list comprehension iterates over each line in the sliced list (lines[1:])

###line.strip():

####line.strip(): Removes leading and trailing whitespace characters, including the newline character (\n), from the current line

###line.strip().split(';'):

####split(';'): Splits the stripped line into a list of substrings using the semicolon (;) as the delimiter.
"""

df #Displaying the new dataframe.

df.columns # Displays the names of the columns

df.shape # Displays the total count of the Rows and Columns of the dataset respectively.

df.info()

"""####The df.info() method in pandas provides a summary of a DataFrame's structure and contents. This method is useful for understanding the basic characteristics of your data, such as the number of rows, the number of columns, and the types of data in each column."""

df.isnull().sum() # Displays the total count of the null valuesin the particular columns.

"""####As we can check there is no null data in the dataset."""

df.dtypes #Displays the type of data.

"""The above data is in the object form, so it becomes necessary to convert the data, so the data is available for the analysis.

"""

df = df.astype(int) #Converting all the columns in one line.

df.dtypes

"""As we can check now the data is now available in int form."""

df.mean() #checking the average of the items.

# Calculate itemset frequency (percentage of transactions where each item was purchased)
itemset_frequency = df.mean() * 100

# Create a bar plot
plt.figure(figsize=(12, 6))
itemset_frequency.sort_values(ascending=False).plot(kind='bar', color='skyblue')
plt.title('Itemset Frequency')
plt.ylabel('Percentage of Transactions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Calculate the size of each itemset (number of items in each transaction)
itemset_sizes = df.sum(axis=1)

# Create a DataFrame for the histogram data
histogram_data = pd.DataFrame({'Itemset Size': itemset_sizes})

# Create an interactive histogram using Plotly Express
fig = px.histogram(histogram_data, x='Itemset Size', nbins=max(itemset_sizes) - 1, labels={'Itemset Size': 'Number of Items in Transaction'})
fig.update_layout(
    title='Itemset Size Distribution',
    xaxis_title='Number of Items in Transaction',
    yaxis_title='Frequency',
    xaxis=dict(showline=True, showgrid=False),
    yaxis=dict(showline=True, showgrid=True, gridwidth=0.5, gridcolor='lightgray'),
    bargap=0.05  # Adjust the gap between bars
)

fig.show()

"""#Frequently purchased items"""

# Compute frequent itemsets using the Apriori algorithm
frequent_itemsets = apriori(df, #df: This is the input DataFrame containing the transactional data
                            min_support = 0.2, #Setting min_support to 0.2 means that only itemsets appearing in at least 20% of the transactions will be considered frequent.
                            max_len = 3, # This limits the maximum length of the itemsets to be considered. In this case, only itemsets with up to 3 items will be generated.
                            use_colnames = True)

frequent_itemsets

# Compute all association rules for frequent_itemsets
rules = association_rules(frequent_itemsets, #This is the input DataFrame containing the frequent itemsets that were previously generated using an algorithm like Apriori. Each row in this DataFrame typically includes an itemset and its corresponding support value.
                            metric = 'support', #This parameter specifies the metric used to evaluate the association rules. In this case, 'support' is chosen as the metric.
                            min_threshold=0.1) #This parameter sets the minimum threshold for the chosen metric. Only the rules that meet or exceed this threshold will be considered.

rules

"""#Rules"""

rules = association_rules(frequent_itemsets, metric = "confidence", min_threshold = 0.5)

rules

lift = association_rules(frequent_itemsets, metric = "lift", min_threshold = 1.3)

lift

# Generate scatterplot confidence versus support
sns.scatterplot(x = "support", y = "confidence", data = rules)
plt.show()

# Generate scatterplot confidence versus support

sns.scatterplot(x = "support", y = "confidence", size= 'leverage',data = rules)
plt.legend(bbox_to_anchor= (1.02, 1), loc='upper left',)
plt.show()

# add extra another rule where support more than 0.2 for given itemset
filtered_rules = rules[(rules['antecedent support'] > 0.02)& #This ensures that the rules include antecedent itemsets that appear in more than 2% of the transactions.
                        (rules['consequent support'] >0.01) & #This ensures that the rules include consequent itemsets that appear in more than 1% of the transactions.
                        (rules['confidence'] > 0.45) & #This ensures that the rules have a confidence level greater than 0.45 (or 45%).
                        (rules['lift'] > 1.0)& #This ensures that the rules have a lift greater than 1.
                        (rules['support']>0.195)] #This ensures that the rules have a support value greater than 0.195 (or 19.5%).

filtered_rules

def rules_to_coordinates(rules): #rules_to_coordinates is a function that takes a DataFrame rules as input.
    rules['antecedent'] = rules['antecedents'].apply(lambda antecedent:list(antecedent)[0])
    #rules['antecedent']: This creates a new column in the DataFrame called 'antecedent'.
    #rules['antecedents']: This column contains sets of antecedents for each rule, such as {('milk',)}.
    #.apply(lambda antecedent: list(antecedent)[0]): This lambda function takes each set from the 'antecedents' column, converts it to a list, and extracts the first item from the list. It assumes that each antecedent set contains only one item.

    rules['consequent'] = rules['consequents'].apply(lambda consequent:list(consequent)[0])
    #rules['consequent']: This creates a new column in the DataFrame called 'consequent'.
    #rules['consequents']: This column contains sets of consequents for each rule, such as {('bread',)}.
    #.apply(lambda consequent: list(consequent)[0]): This lambda function extracts the first item from each consequent set, converting it to a single item.

    rules['rule'] = rules.index
    #rules['rule']: This creates a new column in the DataFrame called 'rule' that contains the index of each row.
    #rules.index: This refers to the index of the DataFrame, providing a unique identifier for each rule.

    return rules[['antecedent','consequent','rule']]
    #rules[['antecedent', 'consequent', 'rule']]: This returns a DataFrame with only the columns 'antecedent', 'consequent', and 'rule'.
    #This filtered DataFrame contains the essential information for each rule and is now ready for visualization or further processing.

from pandas.plotting import parallel_coordinates
# Convert rules into coordinates suitable for use in a parallel coordinates plot
coords = rules_to_coordinates(filtered_rules)
# Generate parallel coordinates plot
plt.figure(figsize=(3,6))
parallel_coordinates(coords, 'rule',colormap = 'ocean')
plt.legend([])
plt.show()

"""From the plot it seems like the bacon, banana, cheese can be used as cross-selling with other products, it also acts as something to be offered with antecedents that is low."""